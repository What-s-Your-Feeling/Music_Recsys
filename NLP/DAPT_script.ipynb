{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044f0969-96f1-4fa1-8fb1-b8273e6ef93d",
   "metadata": {},
   "source": [
    "# DAPT 가사 - 일상어 - 감성어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7e8c6-6229-4475-b3f7-bca2c94209c7",
   "metadata": {},
   "source": [
    "### Install library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f31aa-bb01-4dba-9b48-bd21043c1543",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0622f69e-ee28-4cdc-a935-70ada61945bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(2022)\n",
    "torch.manual_seed(2022)\n",
    "np.random.seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432c0f8c-8ae7-4ad4-96d8-210137c99068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 08:50:45.551250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-17 08:50:45.795834: I tensorflow/core/tpu/tpu_initializer_helper.cc:262] Libtpu path is: libtpu.so\n",
      "I0917 08:50:45.899345334 1419266 ev_epoll1_linux.cc:121]     grpc epoll fd: 61\n",
      "D0917 08:50:45.899362416 1419266 ev_posix.cc:141]            Using polling engine: epoll1\n",
      "D0917 08:50:45.899392480 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"grpclb\"\n",
      "D0917 08:50:45.899400610 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"rls_experimental\"\n",
      "D0917 08:50:45.899408147 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"priority_experimental\"\n",
      "D0917 08:50:45.899411043 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0917 08:50:45.899413729 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"pick_first\"\n",
      "D0917 08:50:45.899416420 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"round_robin\"\n",
      "D0917 08:50:45.899422625 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0917 08:50:45.899433895 1419266 dns_resolver_ares.cc:545]   Using ares dns resolver\n",
      "D0917 08:50:45.899453389 1419266 certificate_provider_registry.cc:39] registering certificate provider factory for \"file_watcher\"\n",
      "D0917 08:50:45.899459696 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"cds_experimental\"\n",
      "D0917 08:50:45.899464807 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0917 08:50:45.899468040 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0917 08:50:45.899470736 1419266 lb_policy_registry.cc:48]   registering LB policy factory for \"xds_cluster_manager_experimental\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[percpu.cc : 535] RAW: rseq syscall failed with errno 22 after membarrier sycall succeeded.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d2281f-ab0a-4004-b904-48d9df88e704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12\n"
     ]
    }
   ],
   "source": [
    "# using TPU through torch\n",
    "import torch_xla\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.utils.serialization as xser\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "print(torch_xla.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4900a9-3448-4d54-b268-423307b9c45c",
   "metadata": {},
   "source": [
    "### TPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8371f2-076f-4bc2-865d-69cc0345e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google cloud project에서 TPU 셋팅\n",
    "\n",
    "# .py로 실행할 때 TPU 셋팅 명령어\n",
    "#!export XRT_TPU_CONFIG=\"localservice;0;localhost:51011\"\n",
    "\n",
    "# 주피터 노트 또는 주피터 랩에서 실행할 때, TPU 셋팅 명령어\n",
    "import os\n",
    "os.environ['XRT_TPU_CONFIG'] = \"localservice;0;localhost:51011\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b99bfd-3518-4d70-879e-809dfadaadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a00c752-682e-4c87-8920-bb06c9e55f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='xla', index=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fabc9b-a803-49b8-80e6-bf31545fad6d",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c1da4b-a213-424b-9196-3889b44eaba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1855389696 bytes == 0x93532000 @  0x7fbf60140680 0x7fbf60161824 0x7fbf60161b8a 0x7fbf041e432e 0x7fbf041cfda2 0x7fbf37593451 0x7fbf4dcb0409 0x7fbf4d9598d5 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x569dba 0x5f6eb3 0x56cc1f 0x5f6cd6 0x56bacd 0x569dba 0x5f6eb3 0x50bc2c 0x5f6082 0x56d2d5 0x569dba 0x50bca0 0x570b26 0x569dba 0x6902a7\n",
      "tcmalloc: large alloc 1855389696 bytes == 0x101ea2000 @  0x7fbf60140680 0x7fbf60161824 0x5fb391 0x7fbf4dcb0422 0x7fbf4d9598d5 0x5f6929 0x5f74f6 0x50c383 0x570b26 0x569dba 0x5f6eb3 0x5f6082 0x56d2d5 0x569dba 0x5f6eb3 0x56cc1f 0x5f6cd6 0x56bacd 0x569dba 0x5f6eb3 0x50bc2c 0x5f6082 0x56d2d5 0x569dba 0x50bca0 0x570b26 0x569dba 0x6902a7 0x6023c4 0x5c6730 0x56bacd\n"
     ]
    }
   ],
   "source": [
    "# huggingface에서 사전에 가사-일상어 DAPT 완료된 모델 load\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"JUNEYEOB/DAPT_batch512_lyric_con\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38431540-fb21-49cc-b2a5-dba25eb801a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>>  KLUE_RoBERTa_large number of parameters : 337M'\n"
     ]
    }
   ],
   "source": [
    "klue_roberta_large_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>>  KLUE_RoBERTa_large number of parameters : {round(klue_roberta_large_parameters)}M'\")\n",
    "\n",
    "# freeze layers\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aee184-4058-4213-a088-db3c2d4d4f81",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580e0afe-16cd-491d-b587-d8d96286dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adbf449-2ac1-464c-b410-7083a763c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"JUNEYEOB/DAPT_batch512_lyric_con\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.21.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05fb239-4821-411d-bc0e-47efe854598f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.tokenize of PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11db27-40eb-4df5-99e9-8528ee61ddb6",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a8fa1ce-ff38-4471-92aa-18052f43a4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82610\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아내가 드디어 출산하게 되어서 정말 신이 나</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야</td>\n",
       "      <td>긴장</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워</td>\n",
       "      <td>긴장</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야 너무 행복해</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이제 곧 은퇴할 시기가 되었어 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를 ...</td>\n",
       "      <td>긴장</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content emotion  label\n",
       "0                           아내가 드디어 출산하게 되어서 정말 신이 나      기쁨      0\n",
       "1             당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야      긴장      1\n",
       "2             고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워      긴장      1\n",
       "3        재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야 너무 행복해      기쁨      0\n",
       "4  이제 곧 은퇴할 시기가 되었어 내가 먼저 은퇴를 하고 육 개월 후에 남편도 은퇴를 ...      긴장      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"sentimental_data.xlsx\")\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35dfcc-f0e2-4906-84dd-2585ca8589a8",
   "metadata": {},
   "source": [
    "### Preprocessing and Count by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf67364-ebc2-45f0-8fda-0f99c016ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.reset_index(drop=True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31aff131-2513-4dd9-8142-9d1a88911b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pro = data[['content', 'label']]\n",
    "data_count = data[['content', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1780b8f-8948-4c7e-b697-9b79ccf0c5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>기쁨</th>\n",
       "      <td>10975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>긴장</th>\n",
       "      <td>19875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>분노</th>\n",
       "      <td>20926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>슬픔</th>\n",
       "      <td>23725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>중립</th>\n",
       "      <td>5020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>평화</th>\n",
       "      <td>2089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content\n",
       "emotion         \n",
       "기쁨         10975\n",
       "긴장         19875\n",
       "분노         20926\n",
       "슬픔         23725\n",
       "중립          5020\n",
       "평화          2089"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count.groupby(by=['emotion']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e6a9d6b-4646-4773-ab0e-2589874f602e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82610"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edc029-fafb-48f0-b315-03921b67ad0d",
   "metadata": {},
   "source": [
    "### Save with utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d58eb56-1dde-4b1d-82ff-866f3bbde08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pro.to_csv('data_pro', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93e78e-793f-438b-910f-4b7a82a1d9a1",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "278c3666-9664-478b-83d2-9dc0337c9cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82610"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_csv('data_pro', sep= ',', index_col = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670249a-a796-4bd8-9a2a-50127f702eea",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5794312b-4b2f-4430-9888-65dd00468576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vazz0901/.local/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# default loading option = \"utf-8\"\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data_pro\",\n",
    "    block_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da532fae-7294-45b8-92fd-625c8376729a",
   "metadata": {},
   "source": [
    "### Define the data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8061ed26-2a40-4b66-a7fe-496fac838ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9e1f6-ab86-44b4-b284-a7db208b09a0",
   "metadata": {},
   "source": [
    "### Model to tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ef1d1de-e33e-4f84-8545-1e62b7f12049",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408beb7f-8a3a-433d-83ba-428c2b33299b",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8402304a-8551-4d24-b09f-87ad17e5883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/vazz0901/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 82611\n",
      "  Num Epochs = 78\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 5:46:16, Epoch 77/78]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.761500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-12000\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-12000/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-12500\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-12500/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_mlm/DAPT_batch512_lyric_con_sent/checkpoint-11500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 20777.952058553696\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "batch_size = 512\n",
    "num_train_epochs = 200\n",
    "trained_model_path = f\"test_mlm/DAPT_batch{batch_size}_lyric_con_sent\"\n",
    "\n",
    "#os.mkdir(trained_model_path)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "'''\n",
    "TrainingArguments parameters\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\n",
    "'''\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trained_model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,                 # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,      # batch size per device during training\n",
    "    save_total_limit=2,\n",
    "    weight_decay = 0.01,\n",
    "    tpu_num_cores = 85,\n",
    "    seed = 2022,\n",
    "    data_seed = 2022,\n",
    "    dataloader_pin_memory = True,\n",
    "    max_steps = 12_500,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93741e-51a6-4f1c-abe5-12d300637c4a",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b2c7611-b81d-4ef1-9f9d-a5b345659fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_mlm/DAPT_batch512_lyric_con_sent\n",
      "Configuration saved in test_mlm/DAPT_batch512_lyric_con_sent/config.json\n",
      "Model weights saved in test_mlm/DAPT_batch512_lyric_con_sent/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60504398-90e8-4f7d-8e1e-e551c7ad27f7",
   "metadata": {},
   "source": [
    "### Upload hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "754cc8ee-04cc-441d-aa55-a4fe5fe05c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vazz0901/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'create_repo': pass token='DAPT_batch512_lyric_con_sent' as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/home/vazz0901/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:681: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/JUNEYEOB/DAPT_batch512_lyric_con_sent into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpr62e0sc7/config.json\n",
      "tcmalloc: large alloc 2190458880 bytes == 0x175f454000 @  0x7fbf60140680 0x7fbf60160da2 0x5f8dfc 0x64f870 0x527012 0x5c64c0 0x5f4cc1 0x5f4f85 0x486664 0x539ccb 0x539bf9 0x66321b 0x53a821 0x53a01f 0x6632cc 0x53a164 0x53a01f 0x66321b 0x53a164 0x53a8d8 0x66134d 0x6615f0 0x505166 0x56bbfa 0x569dba 0x5f6eb3 0x56bacd 0x569dba 0x5f6eb3 0x56bacd 0x569dba\n",
      "Model weights saved in /tmp/tmpr62e0sc7/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ad56916bbc466a90e01e492c023f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/JUNEYEOB/DAPT_batch512_lyric_con_sent\n",
      "   3fe078a..0132ab7  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/JUNEYEOB/DAPT_batch512_lyric_con_sent/commit/0132ab7fcc5b8df7a94031e9b4604e471f65ad5b'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SAVE_REPO = 'DAPT_batch512_lyric_con_sent' # ex) 'my-bert-fine-tuned'\n",
    "HUGGINGFACE_AUTO_TOKEN = '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' # https://huggingface.co/settings/token\n",
    " \n",
    "## Push to huggingface-hub\n",
    "model.push_to_hub(\n",
    "    MODEL_SAVE_REPO, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=HUGGINGFACE_AUTO_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
